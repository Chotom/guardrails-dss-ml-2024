# Keeping LLMs on Track: A Practical Guide to Guardrails

This repository contains resources for the speech _"Keeping LLMs on Track: A Practical Guide to Guardrails -
Techniques, Frameworks, and Use Cases."_ presented at
the [Data Science Summit 2024 Machine Learning Edition](https://ml.dssconf.pl/).

> [!NOTE]
> Tutorial, materials and source code will be available here **after** the conference.

## Abstract

The speech focuses on the challenges of quality and safety in large language model (LLM) outputs and how to
improve them. LLMs hold immense potential, but their outputs can be unreliable. This presentation address this challenge
by introducing various guardrails, practical tools to ensure LLMs generate safe, reliable, and high-quality content.
We'll delve into concerns like hallucinations, data leakage, toxicity etc. demonstrating how guardrails minimize these
risks. By attending, you'll gain a deeper understanding of guardrails and how they can safeguard your LLM applications,
enabling their responsible deployment in real-world scenarios.

## Agenda

1. Introduction (Brief overview and problem definition).
2. Safety issues in generated content.
3. Guardrail: introduction and definition.
4. Demo usage and overview for the selected Guardrails tools.
5. Conclusions and Recommendations.

## Project structure

```
.
â”œâ”€â”€ ðŸ“‚ docs - Directory with tutorial and presented materials.
â”œâ”€â”€ ðŸ“‚ runs - Directory for benchmark results.
â””â”€â”€ ðŸ“‚ src  - Directory with source code.
```
